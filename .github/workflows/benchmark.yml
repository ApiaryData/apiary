name: Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:  # Allow manual trigger

env:
  CARGO_TERM_COLOR: always

permissions:
  contents: write  # Needed for GitHub Pages deployment

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          load: true
          tags: apiary:benchmark
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          pip install pyarrow pandas PyYAML

      - name: Run benchmarks
        run: |
          python3 scripts/run_benchmark.py \
            --docker \
            --image apiary:benchmark \
            --sizes 1000,10000,50000 \
            --output benchmark_results.json

      - name: Display benchmark summary
        if: always()
        run: |
          if [ -f benchmark_results.json ]; then
            echo "## Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Environment" >> $GITHUB_STEP_SUMMARY
            echo "- Platform: ubuntu-latest" >> $GITHUB_STEP_SUMMARY
            echo "- Docker Image: apiary:benchmark" >> $GITHUB_STEP_SUMMARY
            echo "- Date: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Extract and format results using Python
            python3 << 'EOF'
          import json
          import sys
          
          with open('benchmark_results.json', 'r') as f:
              data = json.load(f)
          
          print("### Write Performance\n")
          print("| Dataset Size | Throughput (rows/sec) | Elapsed Time (sec) |")
          print("|--------------|----------------------|-------------------|")
          
          for result in data['results']:
              if result['name'] == 'write_benchmark' and result['success']:
                  metrics = result['metrics']
                  rows = int(metrics.get('rows', 0))
                  throughput = metrics.get('throughput', 0)
                  elapsed = metrics.get('elapsed', 0)
                  print(f"| {rows:,} | {throughput:,.2f} | {elapsed:.4f} |")
          
          print("\n### Query Performance\n")
          print("| Dataset Size | Rows Scanned | Throughput (rows/sec) | Elapsed Time (sec) |")
          print("|--------------|--------------|----------------------|-------------------|")
          
          for result in data['results']:
              if result['name'] == 'query_benchmark' and result['success']:
                  metrics = result['metrics']
                  rows = int(metrics.get('rows_scanned', 0))
                  throughput = metrics.get('throughput', 0)
                  elapsed = metrics.get('elapsed', 0)
                  print(f"| {rows:,} | {rows:,} | {throughput:,.2f} | {elapsed:.4f} |")
          EOF
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: benchmark_results.json
          retention-days: 90

      - name: Check for performance regressions
        if: github.event_name == 'pull_request'
        run: |
          echo "Note: Performance regression detection will be implemented in future PRs"
          echo "For now, manually review the benchmark results artifact"

      - name: Run multi-node benchmarks
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          python3 scripts/run_multinode_benchmark.py \
            --nodes 2 \
            --image apiary:benchmark \
            --sizes 5000,10000 \
            --output multinode_benchmark_results.json

      - name: Display multi-node benchmark summary
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          if [ -f multinode_benchmark_results.json ]; then
            echo "## Multi-Node Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### Configuration" >> $GITHUB_STEP_SUMMARY
            echo "- Nodes: 2" >> $GITHUB_STEP_SUMMARY
            echo "- Storage: MinIO (S3-compatible)" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            python3 << 'EOF'
          import json
          
          with open('multinode_benchmark_results.json', 'r') as f:
              data = json.load(f)
          
          print("### Distributed Write Performance\n")
          print("| Dataset Size | Throughput (rows/sec) | Elapsed Time (sec) | Verified Nodes |")
          print("|--------------|----------------------|-------------------|----------------|")
          
          for result in data['results']:
              if result['name'] == 'distributed_write_benchmark' and result['success']:
                  metrics = result['metrics']
                  rows = int(metrics.get('rows', 0))
                  throughput = metrics.get('throughput', 0)
                  elapsed = metrics.get('elapsed', 0)
                  verified = metrics.get('verified_nodes', 0)
                  print(f"| {rows:,} | {throughput:,.2f} | {elapsed:.4f} | {verified} |")
          
          print("\n### Distributed Query Performance\n")
          print("| Dataset Size | Throughput (rows/sec) | Avg Elapsed (sec) | Nodes Alive | Total Bees |")
          print("|--------------|----------------------|-------------------|-------------|------------|")
          
          for result in data['results']:
              if result['name'] == 'distributed_query_benchmark' and result['success']:
                  metrics = result['metrics']
                  rows = int(metrics.get('rows_queried', 0))
                  throughput = metrics.get('throughput', 0)
                  elapsed = metrics.get('avg_elapsed', 0)
                  nodes = metrics.get('nodes_alive', 0)
                  bees = metrics.get('total_bees', 0)
                  print(f"| {rows:,} | {throughput:,.2f} | {elapsed:.4f} | {nodes} | {bees} |")
          EOF
          fi

      - name: Generate HTML report
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          python3 scripts/generate_benchmark_report.py \
            --single-node benchmark_results.json \
            --multi-node multinode_benchmark_results.json \
            --output benchmark-report/index.html \
            --run-number "${{ github.run_number }}" \
            --commit-sha "${{ github.sha }}" \
            --platform "ubuntu-latest"

      - name: Deploy to GitHub Pages
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./benchmark-report
          commit_message: "Update benchmark results - Run #${{ github.run_number }}"
